"""
STEP 1: DATA ANALYSIS - PhÃ¢n tÃ­ch Dá»¯ liá»‡u Ban Ä‘áº§u
================================
Má»¥c tiÃªu: Hiá»ƒu rÃµ dá»¯ liá»‡u, phÃ¡t hiá»‡n imbalance, vÃ  xÃ¡c Ä‘á»‹nh chiáº¿n lÆ°á»£c
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
import warnings
warnings.filterwarnings('ignore')

# Set style
sns.set_style("whitegrid")
plt.rcParams['figure.figsize'] = (15, 10)

print("=" * 80)
print("BÆ¯á»šC 1: PHÃ‚N TÃCH Dá»® LIá»†U - THE DATA FOUNDATION")
print("=" * 80)

# Load data
print("\nðŸ“Š Loading data.csv...")
df = pd.read_csv('data/data.csv')

print(f"\nâœ… Data loaded successfully!")
print(f"   Shape: {df.shape}")
print(f"   Columns: {df.columns.tolist()}")

# ==================== CRITICAL: CLASS IMBALANCE ANALYSIS ====================
print("\n" + "=" * 80)
print("âš ï¸  PHÃT HIá»†N THEN CHá»T: CLASS IMBALANCE ANALYSIS")
print("=" * 80)

conversion_counts = df['conversion'].value_counts()
conversion_ratio = df['conversion'].value_counts(normalize=True) * 100

print("\nðŸ“ˆ Conversion Distribution:")
print(f"   Class 0 (No Conversion): {conversion_counts[0]:,} ({conversion_ratio[0]:.2f}%)")
print(f"   Class 1 (Conversion):    {conversion_counts[1]:,} ({conversion_ratio[1]:.2f}%)")
print(f"\n   âš ï¸  IMBALANCE RATIO: {conversion_ratio[0] / conversion_ratio[1]:.2f} : 1")
print(f"   âš ï¸  ÄÃ¢y lÃ  dá»¯ liá»‡u Máº¤T CÃ‚N Báº°NG NGHIÃŠM TRá»ŒNG!")

# Visual: Class distribution
fig, axes = plt.subplots(1, 2, figsize=(15, 5))

# Bar plot
conversion_counts.plot(kind='bar', ax=axes[0], color=['#e74c3c', '#2ecc71'])
axes[0].set_title('Class Distribution - Absolute Count', fontsize=14, fontweight='bold')
axes[0].set_xlabel('Conversion (0=No, 1=Yes)', fontsize=12)
axes[0].set_ylabel('Count', fontsize=12)
axes[0].tick_params(rotation=0)

# Pie chart
axes[1].pie(conversion_counts, labels=['No Conversion (0)', 'Conversion (1)'], 
            autopct='%1.1f%%', colors=['#e74c3c', '#2ecc71'], startangle=90)
axes[1].set_title('Class Distribution - Percentage', fontsize=14, fontweight='bold')

plt.tight_layout()
plt.savefig('01_class_imbalance_analysis.png', dpi=300, bbox_inches='tight')
print("\nâœ… Visualization saved: 01_class_imbalance_analysis.png")

# ==================== BASIC STATISTICS ====================
print("\n" + "=" * 80)
print("ðŸ“Š BASIC STATISTICS")
print("=" * 80)

print("\n1. Missing Values Check:")
missing = df.isnull().sum()
if missing.sum() == 0:
    print("   âœ… No missing values detected!")
else:
    print(missing[missing > 0])

print("\n2. Data Types:")
print(df.dtypes)

print("\n3. Numerical Features Statistics:")
print(df.describe())

print("\n4. Categorical Features - Unique Values:")
categorical_cols = df.select_dtypes(include=['object']).columns
for col in categorical_cols:
    print(f"   {col}: {df[col].nunique()} unique values - {df[col].unique()[:5]}")

# ==================== FEATURE ANALYSIS ====================
print("\n" + "=" * 80)
print("ðŸ” FEATURE ANALYSIS BY CONVERSION")
print("=" * 80)

# Numerical features by conversion
numerical_cols = ['recency', 'history']
print("\n1. Numerical Features Statistics by Conversion:")
for col in numerical_cols:
    print(f"\n   {col.upper()}:")
    print(df.groupby('conversion')[col].describe()[['mean', 'std', 'min', 'max']])

# Categorical features by conversion
print("\n2. Categorical Features Distribution by Conversion:")
categorical_analysis = ['offer', 'channel', 'zip_code']
for col in categorical_analysis:
    print(f"\n   {col.upper()}:")
    crosstab = pd.crosstab(df[col], df['conversion'], normalize='index') * 100
    print(crosstab.round(2))

# ==================== CORRELATION ANALYSIS ====================
print("\n" + "=" * 80)
print("ðŸ”— CORRELATION ANALYSIS")
print("=" * 80)

# Prepare data for correlation
df_corr = df.copy()
# Encode binary features
df_corr['used_discount'] = df_corr['used_discount'].astype(int)
df_corr['used_bogo'] = df_corr['used_bogo'].astype(int)
df_corr['is_referral'] = df_corr['is_referral'].astype(int)

# Select numeric columns for correlation
numeric_features = ['recency', 'history', 'used_discount', 'used_bogo', 'is_referral', 'conversion']
correlation_matrix = df_corr[numeric_features].corr()

print("\nðŸ“Š Correlation with Conversion:")
conversion_corr = correlation_matrix['conversion'].sort_values(ascending=False)
print(conversion_corr)

# Visual: Correlation heatmap
plt.figure(figsize=(10, 8))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0, 
            square=True, linewidths=1, cbar_kws={"shrink": 0.8}, fmt='.3f')
plt.title('Correlation Matrix - Original Features', fontsize=14, fontweight='bold')
plt.tight_layout()
plt.savefig('02_correlation_matrix.png', dpi=300, bbox_inches='tight')
print("\nâœ… Visualization saved: 02_correlation_matrix.png")

# ==================== KEY INSIGHTS ====================
print("\n" + "=" * 80)
print("ðŸ’¡ KEY INSIGHTS & STRATEGIC IMPLICATIONS")
print("=" * 80)

print("""
1. âš ï¸  CRITICAL CHALLENGE: Severe Class Imbalance (85.3% : 14.7%)
   â†’ Strategy: SMOTE + ENN resampling + Class Weights required

2. ðŸ“Š Feature Correlation with Conversion:
   â†’ Identify which features have strongest signal
   â†’ Need feature engineering to amplify these signals

3. ðŸŽ¯ Path to F1 > 90%:
   âœ“ Enhanced features with F&B context (seat_usage, time_of_day, etc.)
   âœ“ Interaction features (spending_velocity, context_combo, etc.)
   âœ“ Advanced ensemble (LightGBM + XGBoost + CatBoost + Stacking)
   âœ“ Threshold tuning for optimal F1-score

4. ðŸ“ˆ Next Steps:
   â†’ Step 2: Create enhanced_data.csv (15 columns)
   â†’ Step 3: Advanced feature engineering
   â†’ Step 4: Build "Big 3" models with Optuna tuning
   â†’ Step 5: Stacking ensemble + Meta-model
   â†’ Step 6: Threshold optimization

""")

print("=" * 80)
print("âœ… STEP 1 COMPLETED: Data Analysis Foundation Established")
print("=" * 80)
print("\nðŸ“Š Generated Files:")
print("   - 01_class_imbalance_analysis.png")
print("   - 02_correlation_matrix.png")
print("\nðŸš€ Ready to proceed to Step 2: Enhanced Feature Creation")
