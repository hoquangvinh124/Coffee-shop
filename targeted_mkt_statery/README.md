# Chiáº¿n LÆ°á»£c Marketing CÃ³ Má»¥c TiÃªu cho Starbucks

# Targeted Marketing Strategy for Starbucks

[![Python](https://img.shields.io/badge/Python-3.8+-blue.svg)](https://www.python.org/)
[![TensorFlow](https://img.shields.io/badge/TensorFlow-2.x-orange.svg)](https://www.tensorflow.org/)
[![Scikit-learn](https://img.shields.io/badge/Scikit--learn-1.0+-green.svg)](https://scikit-learn.org/)

## ğŸ“‹ MÃ´ Táº£ Dá»± Ãn

Dá»± Ã¡n Machine Learning dá»± Ä‘oÃ¡n **pháº£n á»©ng cá»§a khÃ¡ch hÃ ng** vá»›i cÃ¡c chÆ°Æ¡ng trÃ¬nh khuyáº¿n mÃ£i (offers) Ä‘Æ°á»£c gá»­i qua á»©ng dá»¥ng Starbucks. ÄÃ¢y lÃ  bÃ i toÃ¡n **multiclass classification** vá»›i 5 loáº¡i pháº£n á»©ng khÃ¡c nhau.

### ğŸ¯ Má»¥c TiÃªu

XÃ¢y dá»±ng model ML Ä‘á»ƒ:

- Dá»± Ä‘oÃ¡n hÃ nh vi khÃ¡ch hÃ ng khi nháº­n offers
- Tá»‘i Æ°u hÃ³a chiáº¿n lÆ°á»£c marketing
- Giáº£m chi phÃ­ báº±ng cÃ¡ch nháº­n diá»‡n "green flag customers" (khÃ´ng cáº§n gá»­i offer)

### ğŸ“Š Target Classes

| Class | Event           | MÃ´ Táº£                          |
| ----- | --------------- | ------------------------------ |
| 0     | Offer Received  | KhÃ¡ch hÃ ng nháº­n Ä‘Æ°á»£c offer     |
| 1     | Offer Viewed    | KhÃ¡ch hÃ ng xem offer           |
| 2     | Transaction     | Giao dá»‹ch khÃ´ng dÃ¹ng offer     |
| 3     | Offer Completed | HoÃ n thÃ nh giao dá»‹ch vá»›i offer |
| 4     | Green Flag      | Giao dá»‹ch mÃ  khÃ´ng cáº§n offer   |

---

## ğŸ“ Cáº¥u TrÃºc Dá»± Ãn

```
targeted_mkt_statery/
â”‚
â”œâ”€â”€ data/                           # Dá»¯ liá»‡u thÃ´ vÃ  Ä‘Ã£ xá»­ lÃ½
â”‚   â”œâ”€â”€ portfolio.json              # ThÃ´ng tin offers
â”‚   â”œâ”€â”€ profile.json                # ThÃ´ng tin khÃ¡ch hÃ ng
â”‚   â”œâ”€â”€ transcript.json             # Lá»‹ch sá»­ tÆ°Æ¡ng tÃ¡c
â”‚   â””â”€â”€ processed/                  # Dá»¯ liá»‡u Ä‘Ã£ preprocessing
â”‚       â”œâ”€â”€ X_train.csv
â”‚       â”œâ”€â”€ X_test.csv
â”‚       â”œâ”€â”€ y_train.csv
â”‚       â”œâ”€â”€ y_test.csv
â”‚       â””â”€â”€ metadata.pkl
â”‚
â”œâ”€â”€ notebooks/                      # Jupyter Notebooks
â”‚   â”œâ”€â”€ 01_data_loading_and_eda.ipynb       # EDA vÃ  phÃ¢n tÃ­ch
â”‚   â”œâ”€â”€ 02_data_preprocessing.ipynb         # Tiá»n xá»­ lÃ½ dá»¯ liá»‡u
â”‚   â”œâ”€â”€ 03_model_training.ipynb             # Training models
â”‚   â””â”€â”€ 04_model_evaluation.ipynb           # ÄÃ¡nh giÃ¡ vÃ  so sÃ¡nh
â”‚
â”œâ”€â”€ src/                            # Source code modules
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ data_loader.py              # Load vÃ  merge data
â”‚   â”œâ”€â”€ preprocessor.py             # Preprocessing pipeline
â”‚   â”œâ”€â”€ models.py                   # Model definitions
â”‚   â””â”€â”€ utils.py                    # Utility functions
â”‚
â”œâ”€â”€ models/                         # Trained models
â”‚   â”œâ”€â”€ dnn_model.h5
â”‚   â”œâ”€â”€ xgboost_model.pkl
â”‚   â””â”€â”€ random_forest_model.pkl
â”‚
â”œâ”€â”€ results/                        # Káº¿t quáº£ vÃ  visualizations
â”‚   â”œâ”€â”€ figures/                    # Biá»ƒu Ä‘á»“, plots
â”‚   â””â”€â”€ metrics/                    # Performance metrics
â”‚
â”œâ”€â”€ config/                         # Configuration files
â”‚   â””â”€â”€ config.yaml
â”‚
â”œâ”€â”€ requirements.txt                # Python dependencies
â””â”€â”€ README.md                       # File nÃ y
```

---

## ğŸš€ HÆ°á»›ng Dáº«n Sá»­ Dá»¥ng

### 1. CÃ i Äáº·t

```bash
# Clone repository
git clone <repo-url>
cd targeted_mkt_statery

# Táº¡o virtual environment (khuyáº¿n nghá»‹)
python -m venv venv
source venv/bin/activate  # Linux/Mac
# hoáº·c
venv\Scripts\activate  # Windows

# CÃ i Ä‘áº·t dependencies
pip install -r requirements.txt
```

### 2. Cháº¡y Notebooks

Thá»±c hiá»‡n theo thá»© tá»±:

#### **Notebook 01 - Data Loading & EDA**

```bash
jupyter notebook notebooks/01_data_loading_and_eda.ipynb
```

- Load vÃ  khÃ¡m phÃ¡ dá»¯ liá»‡u
- PhÃ¢n tÃ­ch thá»‘ng kÃª mÃ´ táº£
- Visualizations

#### **Notebook 02 - Data Preprocessing**

```bash
jupyter notebook notebooks/02_data_preprocessing.ipynb
```

- Xá»­ lÃ½ missing values
- Merge 3 dataframes
- Feature engineering
- Feature encoding & scaling
- Train/Test split

#### **Notebook 03 - Model Training**

```bash
jupyter notebook notebooks/03_model_training.ipynb
```

- Handle imbalanced dataset
- Train DNN, XGBoost, Random Forest
- Hyperparameter tuning
- Save models

#### **Notebook 04 - Model Evaluation**

```bash
jupyter notebook notebooks/04_model_evaluation.ipynb
```

- ÄÃ¡nh giÃ¡ performance
- Confusion matrix analysis
- Feature importance
- SHAP analysis
- Model comparison

---

## ğŸ“Š Dataset

### Portfolio (10 offers)

- **offer_id**: ID cá»§a offer
- **offer_type**: BOGO, Discount, Informational
- **reward**: Pháº§n thÆ°á»Ÿng ($)
- **difficulty**: Sá»‘ tiá»n cáº§n chi Ä‘á»ƒ nháº­n reward
- **duration**: Thá»i háº¡n offer (days)

### Profile (~17K customers)

- **id**: Customer ID
- **gender**: F, M, O
- **age**: Tuá»•i
- **income**: Thu nháº­p hÃ ng nÄƒm ($)
- **became_member_on**: NgÃ y Ä‘Äƒng kÃ½ (YYYYMMDD)

### Transcript (~300K transactions)

- **person**: Customer ID
- **event**: offer received, viewed, completed, transaction
- **value**: Offer ID hoáº·c transaction amount
- **time**: Thá»i gian (hours)

---

## ğŸ¤– Models

### 1. Deep Neural Network (DNN)

- **Architecture**: Multi-input vá»›i Entity Embedding
- **Embedding**: offer_id (200 dims), gender (200 dims)
- **Layers**: Dense(32) â†’ Dense(32) â†’ Dropout(0.2) â†’ Dense(32) â†’ Dense(5)
- **Activation**: ReLU (hidden), Softmax (output)
- **Optimizer**: Adam
- **Loss**: Sparse Categorical Crossentropy

### 2. XGBoost

- **Type**: Gradient Boosting Tree
- **Objective**: multi:softmax
- **Parameters**: max_depth=10, gamma=5
- **Advantages**: Handle imbalanced data tá»‘t hÆ¡n

### 3. Random Forest

- **n_estimators**: 150 trees
- **class_weight**: 'balanced_subsample'
- **Advantages**: Built-in class balancing

---

## âš–ï¸ Handling Imbalanced Dataset

### Problem

Dataset bá»‹ **imbalanced** nghiÃªm trá»ng:

- Class 3 (offer completed): ~40% (majority)
- Class 1 (offer viewed): ~10% (minority)
- Class 4 (green flag): ~5% (minority)

### Solutions

#### 1. Random Over-Sampling (SMOTE)

```python
from imblearn.over_sampling import RandomOverSampler
sm = RandomOverSampler(sampling_strategy='not majority')
X_train, y_train = sm.fit_sample(X_train, y_train)
```

- âœ… Táº¥t cáº£ classes cÃ³ recall ~50%
- âš ï¸ Trade-off: Overall accuracy giáº£m nhÆ°ng fair hÆ¡n

#### 2. Class Weight Adjustment

```python
class_weights = {
    0: 3.2,
    1: 39.0,  # Minority class
    2: 1.4,
    3: 1.0,   # Majority class
    4: 6.0
}
model.fit(..., class_weight=class_weights)
```

---

## ğŸ“ˆ Evaluation Metrics

### Primary Metrics

- **Micro-averaged F1-score**: Best cho imbalanced multiclass
- **Confusion Matrix**: Detailed class-wise performance
- **Recall per class**: Quan trá»ng cho minority classes

### Results Summary

| Model                  | Imbalanced F1 | Balanced F1 | Best For         |
| ---------------------- | ------------- | ----------- | ---------------- |
| DNN (Label Encoded)    | 61.89%        | -           | Baseline         |
| DNN (Entity Embedding) | **63.12%**    | ~50%        | Best DNN         |
| XGBoost                | 63.45%        | ~50%        | **Best Overall** |
| XGBoost (SMOTE)        | ~50%          | ~50%        | Fair prediction  |
| Random Forest          | 63%           | ~50%        | Ensemble         |

### Key Findings

- **XGBoost** performs best trÃªn imbalanced data
- **Entity Embedding** > One-hot encoding cho categorical features
- **SMOTE** giÃºp model há»c fair hÆ¡n cho táº¥t cáº£ classes

---

## ğŸ” Feature Importance (SHAP Analysis)

Top features áº£nh hÆ°á»Ÿng Ä‘áº¿n predictions:

1. **offer_id** - Highest impact (loáº¡i offer quan trá»ng nháº¥t)
2. **income** - Strong predictor
3. **age** - Medium impact
4. **difficulty** - Medium impact
5. **reward** - Medium impact
6. **reg_month** - Lowest impact

---

## ğŸ“ Key Takeaways

### 1. Model Selection

- XGBoost > DNN cho tabular data nÃ y
- Entity Embedding essential cho categorical features trong DNN
- Tree-based models handle imbalanced data tá»‘t hÆ¡n

### 2. Imbalanced Data

- **KHÃ”NG thá»ƒ ignore** minority classes trong business context
- RandomOverSampler trade-off: accuracy â†“ nhÆ°ng fairness â†‘
- Class weights effective nhÆ°ng cáº§n tuning cáº©n tháº­n

### 3. Business Value

```
Scenario 1 (Imbalanced Model):
âœ“ High accuracy (63%)
âœ— Minority customers ignored
âœ— Lost marketing opportunities

Scenario 2 (Balanced Model):
âœ“ Fair prediction (50% all classes)
âœ“ Better customer segmentation
âœ“ Targeted marketing hiá»‡u quáº£ hÆ¡n
```

### 4. Next Steps

- Collect thÃªm data cho minority classes
- Ensemble methods: XGBoost + Random Forest
- Hyperparameter tuning cho oversampled data
- A/B testing trÃªn production data

---

## ğŸ› ï¸ Tech Stack

- **Python** 3.8+
- **Data Processing**: pandas, numpy
- **Visualization**: matplotlib, seaborn
- **ML Libraries**:
  - scikit-learn (preprocessing, Random Forest)
  - XGBoost (gradient boosting)
  - TensorFlow/Keras (Deep Learning)
  - imbalanced-learn (SMOTE)
- **Model Interpretation**: SHAP
- **Development**: Jupyter Notebook

---

## ğŸ‘¥ Contributors

- **Senior Data Scientist**: Project Lead & Development

---

## ğŸ“„ License

This project is for educational purposes.

---

## ğŸ“ Contact

For questions or feedback, please open an issue in the repository.

---

**Happy Modeling! ğŸš€**
