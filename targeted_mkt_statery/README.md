# Chiáº¿n LÆ°á»£c Marketing CÃ³ Má»¥c TiÃªu cho Starbucks

# Targeted Marketing Strategy for Starbucks

[![Python](https://img.shields.io/badge/Python-3.8+-blue.svg)](https://www.python.org/)
[![TensorFlow](https://img.shields.io/badge/TensorFlow-2.x-orange.svg)](https://www.tensorflow.org/)
[![Scikit-learn](https://img.shields.io/badge/Scikit--learn-1.0+-green.svg)](https://scikit-learn.org/)

## ğŸ“‹ MÃ´ Táº£ Dá»± Ãn

Dá»± Ã¡n Machine Learning dá»± Ä‘oÃ¡n **pháº£n á»©ng cá»§a khÃ¡ch hÃ ng** vá»›i cÃ¡c chÆ°Æ¡ng trÃ¬nh khuyáº¿n mÃ£i (offers) Ä‘Æ°á»£c gá»­i qua á»©ng dá»¥ng Starbucks. ÄÃ¢y lÃ  bÃ i toÃ¡n **multiclass classification** vá»›i 5 loáº¡i pháº£n á»©ng khÃ¡c nhau.

### ğŸ¯ Má»¥c TiÃªu

XÃ¢y dá»±ng model ML Ä‘á»ƒ:

- Dá»± Ä‘oÃ¡n hÃ nh vi khÃ¡ch hÃ ng khi nháº­n offers
- Tá»‘i Æ°u hÃ³a chiáº¿n lÆ°á»£c marketing
- Giáº£m chi phÃ­ báº±ng cÃ¡ch nháº­n diá»‡n "green flag customers" (khÃ´ng cáº§n gá»­i offer)

### ğŸ“Š Target Classes

| Class | Event           | MÃ´ Táº£                          |
| ----- | --------------- | ------------------------------ |
| 0     | Offer Received  | KhÃ¡ch hÃ ng nháº­n Ä‘Æ°á»£c offer     |
| 1     | Offer Viewed    | KhÃ¡ch hÃ ng xem offer           |
| 2     | Transaction     | Giao dá»‹ch khÃ´ng dÃ¹ng offer     |
| 3     | Offer Completed | HoÃ n thÃ nh giao dá»‹ch vá»›i offer |
| 4     | Green Flag      | Giao dá»‹ch mÃ  khÃ´ng cáº§n offer   |

---

## ğŸ“ Cáº¥u TrÃºc Dá»± Ãn

```
targeted_mkt_statery/
â”‚
â”œâ”€â”€ data/                           # Dá»¯ liá»‡u thÃ´ vÃ  Ä‘Ã£ xá»­ lÃ½
â”‚   â”œâ”€â”€ portfolio.json              # ThÃ´ng tin offers
â”‚   â”œâ”€â”€ profile.json                # ThÃ´ng tin khÃ¡ch hÃ ng
â”‚   â”œâ”€â”€ transcript.json             # Lá»‹ch sá»­ tÆ°Æ¡ng tÃ¡c
â”‚   â””â”€â”€ processed/                  # Dá»¯ liá»‡u Ä‘Ã£ preprocessing
â”‚       â”œâ”€â”€ X_train.csv
â”‚       â”œâ”€â”€ X_test.csv
â”‚       â”œâ”€â”€ y_train.csv
â”‚       â”œâ”€â”€ y_test.csv
â”‚       â””â”€â”€ metadata.pkl
â”‚
â”œâ”€â”€ notebooks/                      # Jupyter Notebooks
â”‚   â”œâ”€â”€ 01_data_loading_and_eda.ipynb       # EDA vÃ  phÃ¢n tÃ­ch
â”‚   â”œâ”€â”€ 02_data_preprocessing.ipynb         # Tiá»n xá»­ lÃ½ dá»¯ liá»‡u
â”‚   â”œâ”€â”€ 03_model_training.ipynb             # Training models
â”‚   â”œâ”€â”€ 04_model_evaluation.ipynb           # ÄÃ¡nh giÃ¡ vÃ  so sÃ¡nh
â”‚   â”œâ”€â”€ 05_comprehensive_model_testing.ipynb # Testing toÃ n diá»‡n (NEW!)
â”‚   â””â”€â”€ 06_temporal_validation.ipynb        # Temporal validation (NEW!)
â”‚
â”œâ”€â”€ src/                            # Source code modules
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ data_loader.py              # Load vÃ  merge data
â”‚   â”œâ”€â”€ preprocessor.py             # Preprocessing pipeline
â”‚   â”œâ”€â”€ business_evaluator.py       # Business ROI analysis (NEW!)
â”‚   â””â”€â”€ utils.py                    # Utility functions
â”‚
â”œâ”€â”€ scripts/                        # Automation scripts (NEW!)
â”‚   â””â”€â”€ benchmark_models.py         # Production benchmarking
â”‚
â”œâ”€â”€ models/                         # Trained models
â”‚   â”œâ”€â”€ dnn_model.h5
â”‚   â”œâ”€â”€ xgboost_model.pkl
â”‚   â””â”€â”€ random_forest_model.pkl
â”‚
â”œâ”€â”€ results/                        # Káº¿t quáº£ vÃ  visualizations
â”‚   â”œâ”€â”€ figures/                    # Biá»ƒu Ä‘á»“, plots
â”‚   â””â”€â”€ metrics/                    # Performance metrics
â”‚
â”œâ”€â”€ config/                         # Configuration files
â”‚   â””â”€â”€ config.yaml
â”‚
â”œâ”€â”€ requirements.txt                # Python dependencies
â”œâ”€â”€ run_model_testing.ps1          # Testing automation script (NEW!)
â”œâ”€â”€ TESTING_GUIDE.md               # Complete testing guide (NEW!)
â””â”€â”€ README.md                       # File nÃ y
```

---

## ğŸš€ HÆ°á»›ng Dáº«n Sá»­ Dá»¥ng

### 1. CÃ i Äáº·t

```bash
# Clone repository
git clone <repo-url>
cd targeted_mkt_statery

# Táº¡o virtual environment (khuyáº¿n nghá»‹)
python -m venv venv
source venv/bin/activate  # Linux/Mac
# hoáº·c
venv\Scripts\activate  # Windows

# CÃ i Ä‘áº·t dependencies
pip install -r requirements.txt
```

### 2. Cháº¡y Notebooks

Thá»±c hiá»‡n theo thá»© tá»±:

#### **Notebook 01 - Data Loading & EDA**

```bash
jupyter notebook notebooks/01_data_loading_and_eda.ipynb
```

- Load vÃ  khÃ¡m phÃ¡ dá»¯ liá»‡u
- PhÃ¢n tÃ­ch thá»‘ng kÃª mÃ´ táº£
- Visualizations

#### **Notebook 02 - Data Preprocessing**

```bash
jupyter notebook notebooks/02_data_preprocessing.ipynb
```

- Xá»­ lÃ½ missing values
- Merge 3 dataframes
- Feature engineering
- Feature encoding & scaling
- Train/Test split

#### **Notebook 03 - Model Training**

```bash
jupyter notebook notebooks/03_model_training.ipynb
```

- Handle imbalanced dataset
- Train DNN, XGBoost, Random Forest
- Hyperparameter tuning
- Save models

#### **Notebook 04 - Model Evaluation**

```bash
jupyter notebook notebooks/04_model_evaluation.ipynb
```

- ÄÃ¡nh giÃ¡ performance
- Confusion matrix analysis
- Feature importance
- SHAP analysis
- Model comparison

### 3. ğŸ§ª Test Model Effectiveness (NEW!)

Comprehensive testing suite Ä‘á»ƒ Ä‘Ã¡nh giÃ¡ Ä‘áº§y Ä‘á»§ hiá»‡u quáº£ cá»§a models:

#### **Quick Start - Cháº¡y Táº¥t Cáº£ Tests**

```powershell
# Windows PowerShell
.\run_model_testing.ps1
```

Hoáº·c chá»n tá»«ng test riÃªng biá»‡t:

#### **Option 1: Production Benchmarking (Nhanh - 2-5 phÃºt)**

```powershell
python scripts/benchmark_models.py
```

Äo lÆ°á»ng:

- âš¡ Inference speed (predictions/second)
- ğŸ’¾ Memory usage
- â±ï¸ Model loading time
- ğŸ“Š Production readiness assessment

#### **Option 2: Comprehensive Testing (Interactive)**

```bash
jupyter notebook notebooks/05_comprehensive_model_testing.ipynb
```

Bao gá»“m:

- ğŸ“ˆ ROC-AUC curves (multiclass one-vs-rest)
- ğŸ“‰ Precision-Recall curves (minority classes)
- ğŸ¯ Prediction confidence analysis
- ğŸ’° Business ROI metrics & campaign simulation
- ğŸ” Error analysis vá»›i SHAP explanations
- ğŸ“Š 27+ professional visualizations

#### **Option 3: Temporal Validation (Interactive)**

```bash
jupyter notebook notebooks/06_temporal_validation.ipynb
```

Kiá»ƒm tra:

- â° Time-based validation (train on old, test on new customers)
- ğŸ“‰ Performance degradation analysis
- ğŸ”„ Model generalization on future data
- ğŸ“Š Comparison vá»›i random-split models

#### **Xem HÆ°á»›ng Dáº«n Chi Tiáº¿t**

```bash
# Äá»c hÆ°á»›ng dáº«n Ä‘áº§y Ä‘á»§
cat TESTING_GUIDE.md
```

**Testing Results Location:**

- Metrics: `results/metrics/`
- Visualizations: `results/figures/`
- Reports: `results/metrics/*_report.txt`

---

## ğŸ“Š Dataset

### Portfolio (10 offers)

- **offer_id**: ID cá»§a offer
- **offer_type**: BOGO, Discount, Informational
- **reward**: Pháº§n thÆ°á»Ÿng ($)
- **difficulty**: Sá»‘ tiá»n cáº§n chi Ä‘á»ƒ nháº­n reward
- **duration**: Thá»i háº¡n offer (days)

### Profile (~17K customers)

- **id**: Customer ID
- **gender**: F, M, O
- **age**: Tuá»•i
- **income**: Thu nháº­p hÃ ng nÄƒm ($)
- **became_member_on**: NgÃ y Ä‘Äƒng kÃ½ (YYYYMMDD)

### Transcript (~300K transactions)

- **person**: Customer ID
- **event**: offer received, viewed, completed, transaction
- **value**: Offer ID hoáº·c transaction amount
- **time**: Thá»i gian (hours)

---

## ğŸ¤– Models

### 1. Deep Neural Network (DNN)

- **Architecture**: Multi-input vá»›i Entity Embedding
- **Embedding**: offer_id (200 dims), gender (200 dims)
- **Layers**: Dense(32) â†’ Dense(32) â†’ Dropout(0.2) â†’ Dense(32) â†’ Dense(5)
- **Activation**: ReLU (hidden), Softmax (output)
- **Optimizer**: Adam
- **Loss**: Sparse Categorical Crossentropy

### 2. XGBoost

- **Type**: Gradient Boosting Tree
- **Objective**: multi:softmax
- **Parameters**: max_depth=10, gamma=5
- **Advantages**: Handle imbalanced data tá»‘t hÆ¡n

### 3. Random Forest

- **n_estimators**: 150 trees
- **class_weight**: 'balanced_subsample'
- **Advantages**: Built-in class balancing

---

## âš–ï¸ Handling Imbalanced Dataset

### Problem

Dataset bá»‹ **imbalanced** nghiÃªm trá»ng:

- Class 3 (offer completed): ~40% (majority)
- Class 1 (offer viewed): ~10% (minority)
- Class 4 (green flag): ~5% (minority)

### Solutions

#### 1. Random Over-Sampling (SMOTE)

```python
from imblearn.over_sampling import RandomOverSampler
sm = RandomOverSampler(sampling_strategy='not majority')
X_train, y_train = sm.fit_sample(X_train, y_train)
```

- âœ… Táº¥t cáº£ classes cÃ³ recall ~50%
- âš ï¸ Trade-off: Overall accuracy giáº£m nhÆ°ng fair hÆ¡n

#### 2. Class Weight Adjustment

```python
class_weights = {
    0: 3.2,
    1: 39.0,  # Minority class
    2: 1.4,
    3: 1.0,   # Majority class
    4: 6.0
}
model.fit(..., class_weight=class_weights)
```

---

## ğŸ“ˆ Evaluation Metrics

### Primary Metrics

- **Micro-averaged F1-score**: Best cho imbalanced multiclass
- **Confusion Matrix**: Detailed class-wise performance
- **Recall per class**: Quan trá»ng cho minority classes

### Results Summary

| Model                  | Imbalanced F1 | Balanced F1 | Best For         |
| ---------------------- | ------------- | ----------- | ---------------- |
| DNN (Label Encoded)    | 61.89%        | -           | Baseline         |
| DNN (Entity Embedding) | **63.12%**    | ~50%        | Best DNN         |
| XGBoost                | 63.45%        | ~50%        | **Best Overall** |
| XGBoost (SMOTE)        | ~50%          | ~50%        | Fair prediction  |
| Random Forest          | 63%           | ~50%        | Ensemble         |

### Key Findings

- **XGBoost** performs best trÃªn imbalanced data
- **Entity Embedding** > One-hot encoding cho categorical features
- **SMOTE** giÃºp model há»c fair hÆ¡n cho táº¥t cáº£ classes

---

## ğŸ§ª Comprehensive Model Testing

Äá»ƒ Ä‘Ã¡nh giÃ¡ Ä‘áº§y Ä‘á»§ hiá»‡u quáº£ cá»§a models trÆ°á»›c khi deployment, dá»± Ã¡n cung cáº¥p bá»™ testing toÃ n diá»‡n:

### ğŸ“Š Testing Components

#### 1. **Enhanced Performance Metrics**

- âœ… ROC-AUC curves (multiclass one-vs-rest)
- âœ… Precision-Recall curves cho minority classes
- âœ… Prediction confidence distributions
- âœ… Per-class performance analysis

#### 2. **Business Metrics & ROI Analysis**

- ğŸ’° ROI calculation dá»±a trÃªn cost/revenue assumptions
- ğŸ¯ Campaign simulation vá»›i budget constraints
- ğŸ“ˆ Strategy comparison (top probability vs random vs threshold)
- ğŸ’µ Cost-benefit analysis per prediction

**Assumptions (configurable):**

- Offer cost: $2.00 per offer sent
- Completion revenue: $10.00 per completed offer
- View value: $0.50 per offer view
- Transaction value: $5.00 per transaction

#### 3. **Error Analysis**

- ğŸ” Top 100 misclassifications vá»›i feature analysis
- ğŸ“Š Error patterns by class
- ğŸ¯ SHAP explanations cho failed predictions
- ğŸ“‰ Feature comparison: errors vs correct predictions

#### 4. **Production Readiness**

- âš¡ Inference speed (predictions/second) cho batch sizes: 1, 10, 100, 1000, 5000
- ğŸ’¾ Memory usage profiling
- â±ï¸ Model loading time
- ğŸ“Š Throughput testing on full dataset

**Performance Tiers:**

- Excellent: > 1000 pred/sec
- Good: 500-1000 pred/sec
- Acceptable: 100-500 pred/sec
- Needs Optimization: < 100 pred/sec

#### 5. **Temporal Validation**

- â° Time-based splits (train on early customers, test on recent)
- ğŸ“‰ Performance degradation analysis
- ğŸ”„ Realistic production performance estimates
- ğŸ“Š Comparison vá»›i random-split models

### ğŸš€ Quick Testing Guide

#### **Option A: Automated Script (Windows)**

```powershell
.\run_model_testing.ps1
```

Menu-driven interface Ä‘á»ƒ chá»n test options.

#### **Option B: Individual Tests**

**1. Production Benchmarking (Fast - 2-5 min)**

```bash
python scripts/benchmark_models.py
```

Output: `results/metrics/production_benchmarks.csv` & report

**2. Comprehensive Testing (Interactive)**

```bash
jupyter notebook notebooks/05_comprehensive_model_testing.ipynb
```

27+ visualizations saved to `results/figures/`

**3. Temporal Validation (Interactive)**

```bash
jupyter notebook notebooks/06_temporal_validation.ipynb
```

Time-based performance analysis

### ğŸ“ˆ Expected Test Results

#### **Model Performance Summary**

| Model                 | F1-Score | ROI    | Speed (pred/sec) | Minority Detection | Recommendation        |
| --------------------- | -------- | ------ | ---------------- | ------------------ | --------------------- |
| **XGBoost Standard**  | 0.7021   | High   | ~2000+           | âŒ Poor (F1=0.00)  | High-volume campaigns |
| **XGBoost Resampled** | 0.6396   | Medium | ~2000+           | âœ… Good (F1=0.42)  | Balanced detection    |
| **Random Forest**     | 0.5950   | Medium | ~500-1000        | âš ï¸ Moderate        | General purpose       |
| **DNN Entity Embed**  | 0.1883   | Low    | ~100-200         | âŒ Poor            | âŒ Not recommended    |

#### **Business Metrics (Estimated)**

Vá»›i $10,000 campaign budget:

- **Best ROI Model**: XGBoost Standard (~150-200% ROI)
- **Best Balanced**: XGBoost Resampled (~100-150% ROI)
- **Optimal Strategy**: Top probability targeting with target class = Offer Completed

#### **Temporal Validation Findings**

- Performance degradation: 5-15% from train to future test
- Models generalize well to new customers
- Recommend periodic retraining every 2-3 months

### ğŸ“š Complete Testing Documentation

Xem chi tiáº¿t Ä‘áº§y Ä‘á»§ trong **[TESTING_GUIDE.md](TESTING_GUIDE.md)**:

- Detailed metrics explanations
- Interpretation guidelines
- Customization options
- Troubleshooting tips
- Deployment checklist

---

## ğŸ” Feature Importance (SHAP Analysis)

Top features áº£nh hÆ°á»Ÿng Ä‘áº¿n predictions:

1. **offer_id** - Highest impact (loáº¡i offer quan trá»ng nháº¥t)
2. **income** - Strong predictor
3. **age** - Medium impact
4. **difficulty** - Medium impact
5. **reward** - Medium impact
6. **reg_month** - Lowest impact

---

## ğŸ“ Key Takeaways

### 1. Model Selection

- XGBoost > DNN cho tabular data nÃ y
- Entity Embedding essential cho categorical features trong DNN
- Tree-based models handle imbalanced data tá»‘t hÆ¡n

### 2. Imbalanced Data

- **KHÃ”NG thá»ƒ ignore** minority classes trong business context
- RandomOverSampler trade-off: accuracy â†“ nhÆ°ng fairness â†‘
- Class weights effective nhÆ°ng cáº§n tuning cáº©n tháº­n

### 3. Business Value

```
Scenario 1 (Imbalanced Model):
âœ“ High accuracy (63%)
âœ— Minority customers ignored
âœ— Lost marketing opportunities

Scenario 2 (Balanced Model):
âœ“ Fair prediction (50% all classes)
âœ“ Better customer segmentation
âœ“ Targeted marketing hiá»‡u quáº£ hÆ¡n
```

### 4. Next Steps

- Collect thÃªm data cho minority classes
- Ensemble methods: XGBoost + Random Forest
- Hyperparameter tuning cho oversampled data
- A/B testing trÃªn production data

---

## ğŸ› ï¸ Tech Stack

- **Python** 3.8+
- **Data Processing**: pandas, numpy
- **Visualization**: matplotlib, seaborn
- **ML Libraries**:
  - scikit-learn (preprocessing, Random Forest)
  - XGBoost (gradient boosting)
  - TensorFlow/Keras (Deep Learning)
  - imbalanced-learn (SMOTE)
- **Model Interpretation**: SHAP
- **Development**: Jupyter Notebook

---

## ğŸ‘¥ Contributors

- **Senior Data Scientist**: Project Lead & Development

---

## ğŸ“„ License

This project is for educational purposes.

---

## ğŸ“ Contact

For questions or feedback, please open an issue in the repository.

---

**Happy Modeling! ğŸš€**
