{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "669a1451",
   "metadata": {},
   "source": [
    "## 1. Setup and Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e57902d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "import joblib\n",
    "from pathlib import Path\n",
    "\n",
    "# ML libraries\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    confusion_matrix, classification_report,\n",
    "    roc_curve, auc, roc_auc_score,\n",
    "    precision_recall_curve, average_precision_score\n",
    ")\n",
    "from sklearn.preprocessing import label_binarize\n",
    "\n",
    "# Add src to path\n",
    "sys.path.append('../src')\n",
    "from business_evaluator import BusinessEvaluator\n",
    "from utils import *\n",
    "\n",
    "# Plotting style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette('husl')\n",
    "\n",
    "print(\"âœ“ Libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc2e1be9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load test data\n",
    "data_dir = Path('../data/processed')\n",
    "X_test = pd.read_csv(data_dir / 'X_test.csv')\n",
    "y_test = pd.read_csv(data_dir / 'y_test.csv').values.ravel()\n",
    "\n",
    "print(f\"Test data shape: {X_test.shape}\")\n",
    "print(f\"Target distribution:\\n{pd.Series(y_test).value_counts().sort_index()}\")\n",
    "\n",
    "# Class names\n",
    "class_names = {\n",
    "    0: 'Offer Received',\n",
    "    1: 'Offer Viewed',\n",
    "    2: 'Transaction',\n",
    "    3: 'Offer Completed'\n",
    "}\n",
    "\n",
    "print(f\"\\nâœ“ Loaded {len(X_test)} test samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0472a8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all trained models\n",
    "models_dir = Path('../models')\n",
    "models = {}\n",
    "\n",
    "# Load XGBoost Standard\n",
    "try:\n",
    "    models['XGBoost Standard'] = joblib.load(models_dir / 'xgboost_model.pkl')\n",
    "    print(\"âœ“ Loaded XGBoost Standard\")\n",
    "except Exception as e:\n",
    "    print(f\"âœ— Failed to load XGBoost Standard: {e}\")\n",
    "\n",
    "# Load XGBoost Resampled\n",
    "try:\n",
    "    models['XGBoost Resampled'] = joblib.load(models_dir / 'xgboost_resampled_model.pkl')\n",
    "    print(\"âœ“ Loaded XGBoost Resampled\")\n",
    "except Exception as e:\n",
    "    print(f\"âœ— Failed to load XGBoost Resampled: {e}\")\n",
    "\n",
    "# Load Random Forest\n",
    "try:\n",
    "    models['Random Forest'] = joblib.load(models_dir / 'random_forest_model.pkl')\n",
    "    print(\"âœ“ Loaded Random Forest\")\n",
    "except Exception as e:\n",
    "    print(f\"âœ— Failed to load Random Forest: {e}\")\n",
    "\n",
    "# Try loading DNN\n",
    "try:\n",
    "    from tensorflow import keras\n",
    "    models['DNN Entity Embedding'] = keras.models.load_model(models_dir / 'dnn_entity_embedding.h5')\n",
    "    print(\"âœ“ Loaded DNN Entity Embedding\")\n",
    "except Exception as e:\n",
    "    print(f\"âœ— Failed to load DNN: {e}\")\n",
    "\n",
    "print(f\"\\nTotal models loaded: {len(models)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4d98391",
   "metadata": {},
   "source": [
    "## 2. Generate Predictions for All Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3227825",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate predictions and probabilities\n",
    "predictions = {}\n",
    "probabilities = {}\n",
    "\n",
    "for model_name, model in models.items():\n",
    "    print(f\"Generating predictions for {model_name}...\")\n",
    "    \n",
    "    # Predictions\n",
    "    y_pred = model.predict(X_test)\n",
    "    predictions[model_name] = y_pred\n",
    "    \n",
    "    # Probabilities (if available)\n",
    "    if hasattr(model, 'predict_proba'):\n",
    "        y_proba = model.predict_proba(X_test)\n",
    "        probabilities[model_name] = y_proba\n",
    "        print(f\"  â†’ Predictions shape: {y_pred.shape}, Probabilities shape: {y_proba.shape}\")\n",
    "    else:\n",
    "        print(f\"  â†’ Predictions shape: {y_pred.shape} (no probabilities)\")\n",
    "\n",
    "print(\"\\nâœ“ All predictions generated\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "506cabf1",
   "metadata": {},
   "source": [
    "## 3. Basic Performance Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be62786a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate basic metrics for all models\n",
    "basic_metrics = []\n",
    "\n",
    "for model_name, y_pred in predictions.items():\n",
    "    metrics = {\n",
    "        'Model': model_name,\n",
    "        'Accuracy': accuracy_score(y_test, y_pred),\n",
    "        'F1 (Micro)': f1_score(y_test, y_pred, average='micro'),\n",
    "        'F1 (Macro)': f1_score(y_test, y_pred, average='macro'),\n",
    "        'F1 (Weighted)': f1_score(y_test, y_pred, average='weighted'),\n",
    "        'Precision (Weighted)': precision_score(y_test, y_pred, average='weighted', zero_division=0),\n",
    "        'Recall (Weighted)': recall_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "    }\n",
    "    basic_metrics.append(metrics)\n",
    "\n",
    "basic_metrics_df = pd.DataFrame(basic_metrics).sort_values('F1 (Weighted)', ascending=False)\n",
    "print(\"\\nðŸ“Š BASIC PERFORMANCE METRICS\")\n",
    "print(\"=\"*80)\n",
    "display(basic_metrics_df.round(4))\n",
    "\n",
    "# Save to CSV\n",
    "output_dir = Path('../results/metrics')\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "basic_metrics_df.to_csv(output_dir / 'comprehensive_basic_metrics.csv', index=False)\n",
    "print(\"\\nâœ“ Saved to results/metrics/comprehensive_basic_metrics.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1da6bc5e",
   "metadata": {},
   "source": [
    "## 4. Per-Class Performance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bb5f4a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Per-class metrics for each model\n",
    "per_class_results = []\n",
    "\n",
    "for model_name, y_pred in predictions.items():\n",
    "    # Get classification report\n",
    "    report = classification_report(\n",
    "        y_test, y_pred, \n",
    "        target_names=[class_names[i] for i in sorted(class_names.keys())],\n",
    "        output_dict=True,\n",
    "        zero_division=0\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"ðŸ“Š {model_name} - Per-Class Performance\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    for class_id, class_name in class_names.items():\n",
    "        if class_name in report:\n",
    "            metrics = report[class_name]\n",
    "            per_class_results.append({\n",
    "                'Model': model_name,\n",
    "                'Class': class_name,\n",
    "                'Precision': metrics['precision'],\n",
    "                'Recall': metrics['recall'],\n",
    "                'F1-Score': metrics['f1-score'],\n",
    "                'Support': int(metrics['support'])\n",
    "            })\n",
    "            \n",
    "            print(f\"{class_name:20s}: Precision={metrics['precision']:.4f}, \"\n",
    "                  f\"Recall={metrics['recall']:.4f}, F1={metrics['f1-score']:.4f}\")\n",
    "\n",
    "per_class_df = pd.DataFrame(per_class_results)\n",
    "per_class_df.to_csv(output_dir / 'comprehensive_per_class_metrics.csv', index=False)\n",
    "print(\"\\nâœ“ Saved to results/metrics/comprehensive_per_class_metrics.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6d672e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize per-class F1 scores\n",
    "fig, ax = plt.subplots(figsize=(14, 6))\n",
    "\n",
    "# Pivot for grouped bar chart\n",
    "pivot_data = per_class_df.pivot(index='Class', columns='Model', values='F1-Score')\n",
    "pivot_data.plot(kind='bar', ax=ax, width=0.8, edgecolor='black')\n",
    "\n",
    "ax.set_title('Per-Class F1-Score Comparison Across Models', fontsize=16, fontweight='bold')\n",
    "ax.set_xlabel('Class', fontsize=12)\n",
    "ax.set_ylabel('F1-Score', fontsize=12)\n",
    "ax.legend(title='Model', loc='upper right')\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "ax.set_ylim(0, 1.0)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save\n",
    "figures_dir = Path('../results/figures')\n",
    "figures_dir.mkdir(parents=True, exist_ok=True)\n",
    "plt.savefig(figures_dir / 'per_class_f1_comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"âœ“ Saved to results/figures/per_class_f1_comparison.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35dda861",
   "metadata": {},
   "source": [
    "## 5. ROC-AUC Analysis (Multiclass)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44d19422",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC-AUC for models with probability outputs\n",
    "n_classes = len(class_names)\n",
    "y_test_bin = label_binarize(y_test, classes=list(range(n_classes)))\n",
    "\n",
    "roc_auc_results = []\n",
    "\n",
    "for model_name, y_proba in probabilities.items():\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"ðŸ“ˆ {model_name} - ROC-AUC Analysis\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    # Calculate ROC curve for each class\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 12))\n",
    "    axes = axes.ravel()\n",
    "    \n",
    "    for class_id in range(n_classes):\n",
    "        # Calculate ROC curve\n",
    "        fpr, tpr, _ = roc_curve(y_test_bin[:, class_id], y_proba[:, class_id])\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "        \n",
    "        # Store result\n",
    "        roc_auc_results.append({\n",
    "            'Model': model_name,\n",
    "            'Class': class_names[class_id],\n",
    "            'ROC-AUC': roc_auc\n",
    "        })\n",
    "        \n",
    "        # Plot\n",
    "        axes[class_id].plot(fpr, tpr, linewidth=2, label=f'ROC (AUC = {roc_auc:.4f})')\n",
    "        axes[class_id].plot([0, 1], [0, 1], 'k--', linewidth=1, label='Random')\n",
    "        axes[class_id].set_xlabel('False Positive Rate', fontsize=10)\n",
    "        axes[class_id].set_ylabel('True Positive Rate', fontsize=10)\n",
    "        axes[class_id].set_title(f'{class_names[class_id]}', fontsize=12, fontweight='bold')\n",
    "        axes[class_id].legend(loc='lower right')\n",
    "        axes[class_id].grid(alpha=0.3)\n",
    "        \n",
    "        print(f\"{class_names[class_id]:20s}: ROC-AUC = {roc_auc:.4f}\")\n",
    "    \n",
    "    plt.suptitle(f'{model_name} - ROC Curves (One-vs-Rest)', fontsize=16, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save\n",
    "    safe_name = model_name.lower().replace(' ', '_')\n",
    "    plt.savefig(figures_dir / f'roc_curves_{safe_name}.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    # Calculate macro and micro average\n",
    "    macro_auc = roc_auc_score(y_test_bin, y_proba, average='macro')\n",
    "    micro_auc = roc_auc_score(y_test_bin, y_proba, average='micro')\n",
    "    print(f\"\\nMacro-Average ROC-AUC: {macro_auc:.4f}\")\n",
    "    print(f\"Micro-Average ROC-AUC: {micro_auc:.4f}\")\n",
    "\n",
    "# Save ROC-AUC results\n",
    "roc_auc_df = pd.DataFrame(roc_auc_results)\n",
    "roc_auc_df.to_csv(output_dir / 'roc_auc_scores.csv', index=False)\n",
    "print(\"\\nâœ“ Saved ROC-AUC results to results/metrics/roc_auc_scores.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2f95f7e",
   "metadata": {},
   "source": [
    "## 6. Precision-Recall Analysis (Minority Classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2ac7746",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Precision-Recall curves for minority classes (1: Offer Viewed, 3: Offer Completed)\n",
    "minority_classes = [1, 3]  # Offer Viewed and Offer Completed\n",
    "\n",
    "pr_results = []\n",
    "\n",
    "for model_name, y_proba in probabilities.items():\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"ðŸ“Š {model_name} - Precision-Recall for Minority Classes\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    for idx, class_id in enumerate(minority_classes):\n",
    "        # Calculate PR curve\n",
    "        precision, recall, _ = precision_recall_curve(\n",
    "            y_test_bin[:, class_id], \n",
    "            y_proba[:, class_id]\n",
    "        )\n",
    "        avg_precision = average_precision_score(y_test_bin[:, class_id], y_proba[:, class_id])\n",
    "        \n",
    "        # Store result\n",
    "        pr_results.append({\n",
    "            'Model': model_name,\n",
    "            'Class': class_names[class_id],\n",
    "            'Avg Precision': avg_precision\n",
    "        })\n",
    "        \n",
    "        # Plot\n",
    "        axes[idx].plot(recall, precision, linewidth=2, label=f'AP = {avg_precision:.4f}')\n",
    "        axes[idx].set_xlabel('Recall', fontsize=12)\n",
    "        axes[idx].set_ylabel('Precision', fontsize=12)\n",
    "        axes[idx].set_title(f'{class_names[class_id]}', fontsize=12, fontweight='bold')\n",
    "        axes[idx].legend(loc='best')\n",
    "        axes[idx].grid(alpha=0.3)\n",
    "        axes[idx].set_xlim([0.0, 1.0])\n",
    "        axes[idx].set_ylim([0.0, 1.05])\n",
    "        \n",
    "        print(f\"{class_names[class_id]:20s}: Average Precision = {avg_precision:.4f}\")\n",
    "    \n",
    "    plt.suptitle(f'{model_name} - Precision-Recall Curves', fontsize=16, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save\n",
    "    safe_name = model_name.lower().replace(' ', '_')\n",
    "    plt.savefig(figures_dir / f'precision_recall_{safe_name}.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "# Save PR results\n",
    "pr_df = pd.DataFrame(pr_results)\n",
    "pr_df.to_csv(output_dir / 'precision_recall_scores.csv', index=False)\n",
    "print(\"\\nâœ“ Saved PR results to results/metrics/precision_recall_scores.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccc76c57",
   "metadata": {},
   "source": [
    "## 7. Prediction Probability Distribution Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceddbcb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze prediction confidence/probability distributions\n",
    "for model_name, y_proba in probabilities.items():\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"ðŸ“Š {model_name} - Prediction Probability Distribution\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    # Get max probability for each prediction\n",
    "    max_proba = np.max(y_proba, axis=1)\n",
    "    predicted_class = np.argmax(y_proba, axis=1)\n",
    "    \n",
    "    # Statistics\n",
    "    print(f\"\\nConfidence Statistics:\")\n",
    "    print(f\"  Mean confidence: {max_proba.mean():.4f}\")\n",
    "    print(f\"  Median confidence: {np.median(max_proba):.4f}\")\n",
    "    print(f\"  Std confidence: {max_proba.std():.4f}\")\n",
    "    print(f\"  Min confidence: {max_proba.min():.4f}\")\n",
    "    print(f\"  Max confidence: {max_proba.max():.4f}\")\n",
    "    \n",
    "    # Confidence by correctness\n",
    "    y_pred = predictions[model_name]\n",
    "    correct_mask = (y_pred == y_test)\n",
    "    \n",
    "    correct_conf = max_proba[correct_mask]\n",
    "    incorrect_conf = max_proba[~correct_mask]\n",
    "    \n",
    "    print(f\"\\nConfidence by Correctness:\")\n",
    "    print(f\"  Correct predictions: {correct_conf.mean():.4f} Â± {correct_conf.std():.4f}\")\n",
    "    print(f\"  Incorrect predictions: {incorrect_conf.mean():.4f} Â± {incorrect_conf.std():.4f}\")\n",
    "    \n",
    "    # Visualize\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "    \n",
    "    # 1. Overall probability distribution\n",
    "    axes[0].hist(max_proba, bins=50, edgecolor='black', alpha=0.7)\n",
    "    axes[0].axvline(max_proba.mean(), color='red', linestyle='--', linewidth=2, label=f'Mean: {max_proba.mean():.3f}')\n",
    "    axes[0].set_xlabel('Maximum Probability', fontsize=12)\n",
    "    axes[0].set_ylabel('Frequency', fontsize=12)\n",
    "    axes[0].set_title('Overall Confidence Distribution', fontsize=12, fontweight='bold')\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(alpha=0.3)\n",
    "    \n",
    "    # 2. Confidence by correctness\n",
    "    axes[1].hist(correct_conf, bins=30, alpha=0.7, label='Correct', color='green', edgecolor='black')\n",
    "    axes[1].hist(incorrect_conf, bins=30, alpha=0.7, label='Incorrect', color='red', edgecolor='black')\n",
    "    axes[1].set_xlabel('Maximum Probability', fontsize=12)\n",
    "    axes[1].set_ylabel('Frequency', fontsize=12)\n",
    "    axes[1].set_title('Confidence: Correct vs Incorrect', fontsize=12, fontweight='bold')\n",
    "    axes[1].legend()\n",
    "    axes[1].grid(alpha=0.3)\n",
    "    \n",
    "    # 3. Box plot by predicted class\n",
    "    conf_by_class = [max_proba[predicted_class == i] for i in range(n_classes)]\n",
    "    axes[2].boxplot(conf_by_class, labels=[class_names[i] for i in range(n_classes)])\n",
    "    axes[2].set_xlabel('Predicted Class', fontsize=12)\n",
    "    axes[2].set_ylabel('Confidence', fontsize=12)\n",
    "    axes[2].set_title('Confidence by Predicted Class', fontsize=12, fontweight='bold')\n",
    "    axes[2].tick_params(axis='x', rotation=45)\n",
    "    axes[2].grid(alpha=0.3)\n",
    "    \n",
    "    plt.suptitle(f'{model_name} - Confidence Analysis', fontsize=16, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save\n",
    "    safe_name = model_name.lower().replace(' ', '_')\n",
    "    plt.savefig(figures_dir / f'confidence_analysis_{safe_name}.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4498bc1a",
   "metadata": {},
   "source": [
    "## 8. Business Metrics & ROI Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b10c15e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Business Evaluator\n",
    "# Assumptions: $2 per offer sent, $10 revenue per completed offer\n",
    "evaluator = BusinessEvaluator(\n",
    "    offer_cost=2.0,\n",
    "    completion_revenue=10.0,\n",
    "    view_value=0.5,\n",
    "    transaction_value=5.0\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ðŸ’° BUSINESS METRICS & ROI ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nCost Assumptions:\")\n",
    "print(f\"  - Offer cost: ${evaluator.offer_cost}\")\n",
    "print(f\"  - Completion revenue: ${evaluator.completion_revenue}\")\n",
    "print(f\"  - View value: ${evaluator.view_value}\")\n",
    "print(f\"  - Transaction value: ${evaluator.transaction_value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b337cd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate ROI for each model\n",
    "roi_results = []\n",
    "\n",
    "for model_name, y_pred in predictions.items():\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"ðŸ’° {model_name} - ROI Analysis\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    roi_metrics = evaluator.calculate_roi(y_test, y_pred, return_details=True)\n",
    "    \n",
    "    print(f\"\\nFinancial Metrics:\")\n",
    "    print(f\"  Total Cost: ${roi_metrics['total_cost']:,.2f}\")\n",
    "    print(f\"  Total Revenue: ${roi_metrics['total_revenue']:,.2f}\")\n",
    "    print(f\"  Net Profit: ${roi_metrics['net_profit']:,.2f}\")\n",
    "    print(f\"  ROI: {roi_metrics['roi_percentage']:.2f}%\")\n",
    "    print(f\"  Cost per Prediction: ${roi_metrics['cost_per_prediction']:.4f}\")\n",
    "    print(f\"  Revenue per Prediction: ${roi_metrics['revenue_per_prediction']:.4f}\")\n",
    "    \n",
    "    roi_results.append({\n",
    "        'Model': model_name,\n",
    "        'Total_Cost': roi_metrics['total_cost'],\n",
    "        'Total_Revenue': roi_metrics['total_revenue'],\n",
    "        'Net_Profit': roi_metrics['net_profit'],\n",
    "        'ROI_%': roi_metrics['roi_percentage'],\n",
    "        'Cost_per_Prediction': roi_metrics['cost_per_prediction'],\n",
    "        'Revenue_per_Prediction': roi_metrics['revenue_per_prediction']\n",
    "    })\n",
    "\n",
    "# Create DataFrame and save\n",
    "roi_df = pd.DataFrame(roi_results).sort_values('ROI_%', ascending=False)\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ðŸ“Š ROI COMPARISON\")\n",
    "print(\"=\"*80)\n",
    "display(roi_df.round(2))\n",
    "\n",
    "roi_df.to_csv(output_dir / 'business_roi_metrics.csv', index=False)\n",
    "print(\"\\nâœ“ Saved to results/metrics/business_roi_metrics.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e44d710",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize ROI comparison\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# ROI percentage\n",
    "roi_df.plot(x='Model', y='ROI_%', kind='bar', ax=axes[0], color='skyblue', edgecolor='black', legend=False)\n",
    "axes[0].set_title('ROI by Model', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xlabel('Model', fontsize=12)\n",
    "axes[0].set_ylabel('ROI (%)', fontsize=12)\n",
    "axes[0].axhline(y=0, color='red', linestyle='--', linewidth=1)\n",
    "axes[0].grid(axis='y', alpha=0.3)\n",
    "axes[0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Net profit\n",
    "roi_df.plot(x='Model', y='Net_Profit', kind='bar', ax=axes[1], color='lightgreen', edgecolor='black', legend=False)\n",
    "axes[1].set_title('Net Profit by Model', fontsize=14, fontweight='bold')\n",
    "axes[1].set_xlabel('Model', fontsize=12)\n",
    "axes[1].set_ylabel('Net Profit ($)', fontsize=12)\n",
    "axes[1].axhline(y=0, color='red', linestyle='--', linewidth=1)\n",
    "axes[1].grid(axis='y', alpha=0.3)\n",
    "axes[1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(figures_dir / 'business_roi_comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"âœ“ Saved to results/figures/business_roi_comparison.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f50ac162",
   "metadata": {},
   "source": [
    "## 9. Campaign Simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70ba69ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate marketing campaigns with different strategies\n",
    "campaign_budget = 10000  # $10,000 budget\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"ðŸŽ¯ MARKETING CAMPAIGN SIMULATION (Budget: ${campaign_budget:,})\")\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "# Compare strategies for models with probabilities\n",
    "comparison_results = evaluator.compare_strategies(\n",
    "    X_test,\n",
    "    y_test,\n",
    "    {name: models[name] for name in probabilities.keys()},\n",
    "    budget=campaign_budget,\n",
    "    strategies=['top_probability', 'random'],\n",
    "    target_classes=[None, 3]  # None = best class, 3 = Offer Completed\n",
    ")\n",
    "\n",
    "print(\"\\nðŸ“Š Campaign Simulation Results:\")\n",
    "display(comparison_results.round(2))\n",
    "\n",
    "# Save results\n",
    "comparison_results.to_csv(output_dir / 'campaign_simulation_results.csv', index=False)\n",
    "print(\"\\nâœ“ Saved to results/metrics/campaign_simulation_results.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7de2c000",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize campaign comparison\n",
    "evaluator.plot_roi_comparison(\n",
    "    comparison_results,\n",
    "    save_path=figures_dir / 'campaign_roi_comparison.png'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d9851ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate business report\n",
    "report_text = evaluator.generate_business_report(\n",
    "    comparison_results,\n",
    "    save_path=output_dir / 'business_evaluation_report.txt'\n",
    ")\n",
    "\n",
    "print(report_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a350a884",
   "metadata": {},
   "source": [
    "## 10. Error Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f567860",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Focus on best model for detailed error analysis\n",
    "best_model_name = basic_metrics_df.iloc[0]['Model']\n",
    "best_y_pred = predictions[best_model_name]\n",
    "best_y_proba = probabilities.get(best_model_name)\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"ðŸ” ERROR ANALYSIS: {best_model_name}\")\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "# Identify misclassifications\n",
    "incorrect_mask = (best_y_pred != y_test)\n",
    "correct_mask = ~incorrect_mask\n",
    "\n",
    "n_errors = incorrect_mask.sum()\n",
    "error_rate = n_errors / len(y_test) * 100\n",
    "\n",
    "print(f\"\\nTotal Errors: {n_errors:,} ({error_rate:.2f}%)\")\n",
    "print(f\"Total Correct: {correct_mask.sum():,} ({100-error_rate:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe90ca96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze error patterns\n",
    "error_data = pd.DataFrame({\n",
    "    'true_class': y_test[incorrect_mask],\n",
    "    'pred_class': best_y_pred[incorrect_mask],\n",
    "    'max_proba': np.max(best_y_proba[incorrect_mask], axis=1) if best_y_proba is not None else np.nan\n",
    "})\n",
    "\n",
    "# Add feature data\n",
    "error_features = X_test[incorrect_mask].reset_index(drop=True)\n",
    "error_data = pd.concat([error_data.reset_index(drop=True), error_features], axis=1)\n",
    "\n",
    "print(\"\\nðŸ“Š Error Distribution by True Class:\")\n",
    "error_by_true = error_data['true_class'].value_counts().sort_index()\n",
    "for class_id, count in error_by_true.items():\n",
    "    class_total = (y_test == class_id).sum()\n",
    "    error_pct = count / class_total * 100\n",
    "    print(f\"{class_names[class_id]:20s}: {count:,} errors ({error_pct:.2f}% of class)\")\n",
    "\n",
    "print(\"\\nðŸ“Š Error Distribution by Predicted Class:\")\n",
    "error_by_pred = error_data['pred_class'].value_counts().sort_index()\n",
    "for class_id, count in error_by_pred.items():\n",
    "    print(f\"{class_names[class_id]:20s}: {count:,} wrong predictions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73fd30b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix for errors\n",
    "error_confusion = pd.crosstab(\n",
    "    error_data['true_class'].map(class_names),\n",
    "    error_data['pred_class'].map(class_names),\n",
    "    rownames=['True Class'],\n",
    "    colnames=['Predicted Class']\n",
    ")\n",
    "\n",
    "print(\"\\nðŸ“Š Error Confusion Matrix:\")\n",
    "display(error_confusion)\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(error_confusion, annot=True, fmt='d', cmap='Reds', cbar_kws={'label': 'Count'})\n",
    "plt.title(f'{best_model_name} - Error Patterns', fontsize=16, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig(figures_dir / f'error_confusion_matrix.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"âœ“ Saved to results/figures/error_confusion_matrix.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c3d280c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature analysis of errors vs correct predictions\n",
    "print(\"\\nðŸ“Š Feature Comparison: Errors vs Correct Predictions\\n\")\n",
    "\n",
    "correct_features = X_test[correct_mask]\n",
    "error_features = X_test[incorrect_mask]\n",
    "\n",
    "feature_comparison = []\n",
    "\n",
    "for col in X_test.columns:\n",
    "    correct_mean = correct_features[col].mean()\n",
    "    error_mean = error_features[col].mean()\n",
    "    diff = error_mean - correct_mean\n",
    "    diff_pct = (diff / correct_mean * 100) if correct_mean != 0 else 0\n",
    "    \n",
    "    feature_comparison.append({\n",
    "        'Feature': col,\n",
    "        'Correct_Mean': correct_mean,\n",
    "        'Error_Mean': error_mean,\n",
    "        'Difference': diff,\n",
    "        'Diff_%': diff_pct\n",
    "    })\n",
    "\n",
    "feature_comp_df = pd.DataFrame(feature_comparison)\n",
    "feature_comp_df = feature_comp_df.sort_values('Diff_%', key=abs, ascending=False)\n",
    "\n",
    "display(feature_comp_df.round(4))\n",
    "\n",
    "feature_comp_df.to_csv(output_dir / 'error_feature_analysis.csv', index=False)\n",
    "print(\"\\nâœ“ Saved to results/metrics/error_feature_analysis.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cb8eba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save top 100 error cases for manual inspection\n",
    "if best_y_proba is not None:\n",
    "    # Sort by confidence (most confident errors first)\n",
    "    error_data_sorted = error_data.sort_values('max_proba', ascending=False).head(100)\n",
    "else:\n",
    "    error_data_sorted = error_data.head(100)\n",
    "\n",
    "# Map class IDs to names\n",
    "error_data_sorted['true_class_name'] = error_data_sorted['true_class'].map(class_names)\n",
    "error_data_sorted['pred_class_name'] = error_data_sorted['pred_class'].map(class_names)\n",
    "\n",
    "error_data_sorted.to_csv(output_dir / 'top_100_errors.csv', index=False)\n",
    "print(\"\\nâœ“ Saved top 100 errors to results/metrics/top_100_errors.csv\")\n",
    "print(\"\\nSample of top errors:\")\n",
    "display(error_data_sorted[['true_class_name', 'pred_class_name', 'max_proba']].head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50bbf7a2",
   "metadata": {},
   "source": [
    "## 11. Summary & Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b45d5249",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate comprehensive summary\n",
    "summary_lines = [\n",
    "    \"\\n\" + \"=\"*80,\n",
    "    \"ðŸ“Š COMPREHENSIVE MODEL TESTING SUMMARY\",\n",
    "    \"=\"*80,\n",
    "    \"\",\n",
    "    \"1. BEST OVERALL MODEL:\",\n",
    "    f\"   Model: {basic_metrics_df.iloc[0]['Model']}\",\n",
    "    f\"   Accuracy: {basic_metrics_df.iloc[0]['Accuracy']:.4f}\",\n",
    "    f\"   F1-Score (Weighted): {basic_metrics_df.iloc[0]['F1 (Weighted)']:.4f}\",\n",
    "    f\"   ROI: {roi_df.iloc[0]['ROI_%']:.2f}%\",\n",
    "    f\"   Net Profit: ${roi_df.iloc[0]['Net_Profit']:,.2f}\",\n",
    "    \"\",\n",
    "    \"2. MINORITY CLASS PERFORMANCE:\",\n",
    "]\n",
    "\n",
    "# Minority class summary\n",
    "for class_id in [1, 3]:\n",
    "    class_data = per_class_df[\n",
    "        (per_class_df['Class'] == class_names[class_id]) & \n",
    "        (per_class_df['Model'] == best_model_name)\n",
    "    ]\n",
    "    if len(class_data) > 0:\n",
    "        row = class_data.iloc[0]\n",
    "        summary_lines.append(\n",
    "            f\"   {class_names[class_id]}: F1={row['F1-Score']:.4f}, \"\n",
    "            f\"Precision={row['Precision']:.4f}, Recall={row['Recall']:.4f}\"\n",
    "        )\n",
    "\n",
    "summary_lines.extend([\n",
    "    \"\",\n",
    "    \"3. BEST BUSINESS STRATEGY:\",\n",
    "    f\"   Strategy: {comparison_results.iloc[0]['strategy']}\",\n",
    "    f\"   Target: {comparison_results.iloc[0]['target_class']}\",\n",
    "    f\"   Expected ROI: {comparison_results.iloc[0]['roi_%']:.2f}%\",\n",
    "    f\"   Conversion Rate: {comparison_results.iloc[0]['conversion_rate_%']:.2f}%\",\n",
    "    \"\",\n",
    "    \"4. KEY FINDINGS:\",\n",
    "    f\"   - Total models tested: {len(models)}\",\n",
    "    f\"   - Best model for accuracy: {basic_metrics_df.iloc[0]['Model']}\",\n",
    "    f\"   - Best model for ROI: {roi_df.iloc[0]['Model']}\",\n",
    "    f\"   - Error rate: {error_rate:.2f}%\",\n",
    "    f\"   - Most challenging class: {error_by_true.idxmax()} (Class {error_by_true.index[error_by_true.argmax()]})\",\n",
    "    \"\",\n",
    "    \"5. RECOMMENDATIONS:\",\n",
    "])\n",
    "\n",
    "# Add recommendations based on results\n",
    "best_roi = roi_df.iloc[0]['ROI_%']\n",
    "if best_roi > 100:\n",
    "    summary_lines.append(\"   âœ“ STRONGLY RECOMMENDED for production deployment\")\n",
    "    summary_lines.append(f\"     High positive ROI ({best_roi:.2f}%) ensures profitability\")\n",
    "elif best_roi > 50:\n",
    "    summary_lines.append(\"   âœ“ RECOMMENDED for production deployment\")\n",
    "    summary_lines.append(f\"     Positive ROI ({best_roi:.2f}%) with room for optimization\")\n",
    "elif best_roi > 0:\n",
    "    summary_lines.append(\"   âš  CAUTIOUSLY RECOMMENDED\")\n",
    "    summary_lines.append(f\"     Low ROI ({best_roi:.2f}%) - consider improving model or adjusting strategy\")\n",
    "else:\n",
    "    summary_lines.append(\"   âœ— NOT RECOMMENDED for deployment\")\n",
    "    summary_lines.append(f\"     Negative ROI ({best_roi:.2f}%) - model needs improvement\")\n",
    "\n",
    "summary_lines.extend([\n",
    "    \"\",\n",
    "    \"6. NEXT STEPS:\",\n",
    "    \"   - Review error cases in results/metrics/top_100_errors.csv\",\n",
    "    \"   - Consider ensemble methods combining multiple models\",\n",
    "    \"   - Implement temporal validation (train on old data, test on new)\",\n",
    "    \"   - Run production benchmarks for deployment planning\",\n",
    "    \"   - A/B test different strategies in real campaigns\",\n",
    "    \"\",\n",
    "    \"=\"*80,\n",
    "    \"âœ“ TESTING COMPLETE\",\n",
    "    \"=\"*80\n",
    "])\n",
    "\n",
    "summary_text = \"\\n\".join(summary_lines)\n",
    "print(summary_text)\n",
    "\n",
    "# Save summary\n",
    "with open(output_dir / 'comprehensive_testing_summary.txt', 'w') as f:\n",
    "    f.write(summary_text)\n",
    "\n",
    "print(\"\\nâœ“ Saved summary to results/metrics/comprehensive_testing_summary.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ea61c4a",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This notebook has provided comprehensive testing of all trained models including:\n",
    "\n",
    "âœ… **Enhanced Performance Metrics**: ROC-AUC, Precision-Recall curves for all classes\n",
    "\n",
    "âœ… **Business Evaluation**: ROI analysis, cost-benefit calculations, campaign simulations\n",
    "\n",
    "âœ… **Error Analysis**: Deep-dive into misclassifications and error patterns\n",
    "\n",
    "âœ… **Confidence Analysis**: Prediction probability distributions and reliability assessment\n",
    "\n",
    "All results have been saved to:\n",
    "- **Metrics**: `results/metrics/`\n",
    "- **Figures**: `results/figures/`\n",
    "\n",
    "**Next Steps**:\n",
    "1. Run `scripts/benchmark_models.py` for production readiness testing\n",
    "2. Review `notebooks/06_temporal_validation.ipynb` for time-based validation\n",
    "3. Implement A/B testing framework for real-world validation"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
