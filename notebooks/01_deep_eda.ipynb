{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 1 Day 1-2: Deep Exploratory Data Analysis\n",
    "## Coffee Shop Sales Time Series Forecasting\n",
    "\n",
    "**Objectives:**\n",
    "- Load and understand the dataset\n",
    "- Aggregate transactions to daily revenue time series\n",
    "- Decompose: trend + seasonality + residual\n",
    "- Stationarity tests (ADF, KPSS)\n",
    "- ACF/PACF analysis\n",
    "- Pattern discovery: hourly, daily, weekly, monthly\n",
    "- Store-level and product category analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Statistical tests\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from statsmodels.tsa.stattools import adfuller, kpss, acf, pacf\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "from scipy import stats\n",
    "\n",
    "# Display settings\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette('husl')\n",
    "\n",
    "print('Libraries imported successfully!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Loading and Initial Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "df = pd.read_excel('../data/raw/Coffee Shop Sales.xlsx')\n",
    "\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"\\nFirst few rows:\")\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data info\n",
    "print(\"Dataset Information:\")\n",
    "print(\"=\"*50)\n",
    "df.info()\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Missing Values:\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Duplicate Rows:\", df.duplicated().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic statistics\n",
    "print(\"Numerical Columns Statistics:\")\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorical columns\n",
    "print(\"Categorical Columns Summary:\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "categorical_cols = df.select_dtypes(include=['object']).columns\n",
    "for col in categorical_cols:\n",
    "    print(f\"\\n{col}:\")\n",
    "    print(df[col].value_counts().head(10))\n",
    "    print(f\"Unique values: {df[col].nunique()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Preprocessing and Time Series Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse dates and times\n",
    "# Identify date and time columns\n",
    "print(\"Column names:\")\n",
    "print(df.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create datetime column (adjust column names based on actual data)\n",
    "# Assuming there's a transaction_date and transaction_time column\n",
    "# This will be adjusted after seeing the actual column names\n",
    "\n",
    "# Example preprocessing:\n",
    "# df['transaction_datetime'] = pd.to_datetime(df['transaction_date'].astype(str) + ' ' + df['transaction_time'].astype(str))\n",
    "# df['date'] = pd.to_datetime(df['transaction_date'])\n",
    "# df['hour'] = df['transaction_datetime'].dt.hour\n",
    "# df['day_of_week'] = df['date'].dt.day_name()\n",
    "# df['month'] = df['date'].dt.month\n",
    "# df['week'] = df['date'].dt.isocalendar().week"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate to daily revenue - PRIMARY TIME SERIES\n",
    "# This will be adjusted based on actual column names\n",
    "# daily_revenue = df.groupby('date').agg({\n",
    "#     'revenue': 'sum',\n",
    "#     'transaction_id': 'count'\n",
    "# }).reset_index()\n",
    "# daily_revenue.columns = ['date', 'revenue', 'transaction_count']\n",
    "# daily_revenue = daily_revenue.set_index('date').sort_index()\n",
    "\n",
    "# print(f\"Date range: {daily_revenue.index.min()} to {daily_revenue.index.max()}\")\n",
    "# print(f\"Total days: {len(daily_revenue)}\")\n",
    "# print(f\"\\nDaily Revenue Summary:\")\n",
    "# print(daily_revenue.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Time Series Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot daily revenue time series\n",
    "# This will be implemented after data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Time Series Decomposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seasonal decomposition\n",
    "# decomposition = seasonal_decompose(daily_revenue['revenue'], model='additive', period=7)\n",
    "\n",
    "# fig, axes = plt.subplots(4, 1, figsize=(15, 10))\n",
    "# decomposition.observed.plot(ax=axes[0], title='Observed')\n",
    "# decomposition.trend.plot(ax=axes[1], title='Trend')\n",
    "# decomposition.seasonal.plot(ax=axes[2], title='Seasonal')\n",
    "# decomposition.resid.plot(ax=axes[3], title='Residual')\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Stationarity Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_stationarity(timeseries, title='Time Series'):\n",
    "    \"\"\"\n",
    "    Perform ADF and KPSS tests for stationarity\n",
    "    \"\"\"\n",
    "    print(f\"\\nStationarity Tests for {title}\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Augmented Dickey-Fuller test\n",
    "    print('\\n1. Augmented Dickey-Fuller Test:')\n",
    "    adf_result = adfuller(timeseries.dropna())\n",
    "    print(f'   ADF Statistic: {adf_result[0]:.4f}')\n",
    "    print(f'   p-value: {adf_result[1]:.4f}')\n",
    "    print(f'   Critical Values:')\n",
    "    for key, value in adf_result[4].items():\n",
    "        print(f'      {key}: {value:.4f}')\n",
    "    \n",
    "    if adf_result[1] <= 0.05:\n",
    "        print(\"   => Series is STATIONARY (reject H0)\")\n",
    "    else:\n",
    "        print(\"   => Series is NON-STATIONARY (fail to reject H0)\")\n",
    "    \n",
    "    # KPSS test\n",
    "    print('\\n2. KPSS Test:')\n",
    "    kpss_result = kpss(timeseries.dropna(), regression='ct')\n",
    "    print(f'   KPSS Statistic: {kpss_result[0]:.4f}')\n",
    "    print(f'   p-value: {kpss_result[1]:.4f}')\n",
    "    print(f'   Critical Values:')\n",
    "    for key, value in kpss_result[3].items():\n",
    "        print(f'      {key}: {value:.4f}')\n",
    "    \n",
    "    if kpss_result[1] <= 0.05:\n",
    "        print(\"   => Series is NON-STATIONARY (reject H0)\")\n",
    "    else:\n",
    "        print(\"   => Series is STATIONARY (fail to reject H0)\")\n",
    "\n",
    "# Test will be run after data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. ACF and PACF Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ACF and PACF plots\n",
    "# fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "# plot_acf(daily_revenue['revenue'].dropna(), lags=40, ax=axes[0])\n",
    "# plot_pacf(daily_revenue['revenue'].dropna(), lags=40, ax=axes[1])\n",
    "# axes[0].set_title('Autocorrelation Function (ACF)')\n",
    "# axes[1].set_title('Partial Autocorrelation Function (PACF)')\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Pattern Discovery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hourly patterns\n",
    "# hourly_revenue = df.groupby('hour')['revenue'].sum().reset_index()\n",
    "# plt.figure(figsize=(12, 5))\n",
    "# plt.bar(hourly_revenue['hour'], hourly_revenue['revenue'])\n",
    "# plt.xlabel('Hour of Day')\n",
    "# plt.ylabel('Total Revenue')\n",
    "# plt.title('Revenue by Hour of Day')\n",
    "# plt.grid(alpha=0.3)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Day of week patterns\n",
    "# dow_order = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
    "# dow_revenue = df.groupby('day_of_week')['revenue'].sum().reindex(dow_order)\n",
    "# plt.figure(figsize=(12, 5))\n",
    "# dow_revenue.plot(kind='bar')\n",
    "# plt.xlabel('Day of Week')\n",
    "# plt.ylabel('Total Revenue')\n",
    "# plt.title('Revenue by Day of Week')\n",
    "# plt.xticks(rotation=45)\n",
    "# plt.grid(alpha=0.3)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Store-Level Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store comparison\n",
    "# store_daily = df.groupby(['date', 'store_location'])['revenue'].sum().reset_index()\n",
    "# fig = px.line(store_daily, x='date', y='revenue', color='store_location',\n",
    "#               title='Daily Revenue by Store Location')\n",
    "# fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Product Category Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Product category revenue\n",
    "# category_revenue = df.groupby('product_category')['revenue'].sum().sort_values(ascending=False)\n",
    "# plt.figure(figsize=(12, 6))\n",
    "# category_revenue.plot(kind='barh')\n",
    "# plt.xlabel('Total Revenue')\n",
    "# plt.ylabel('Product Category')\n",
    "# plt.title('Revenue by Product Category')\n",
    "# plt.grid(alpha=0.3)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Key Insights Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics and insights will be documented here after analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Save Processed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save daily revenue time series for next steps\n",
    "# daily_revenue.to_csv('../data/processed/daily_revenue.csv')\n",
    "# print('Daily revenue time series saved to data/processed/daily_revenue.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
